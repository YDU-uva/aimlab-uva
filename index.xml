<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AIM Lab</title>
    <link>https://ivi.fnwi.uva.nl/aimlab/</link>
      <atom:link href="https://ivi.fnwi.uva.nl/aimlab/index.xml" rel="self" type="application/rss+xml" />
    <description>AIM Lab</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© AIM Lab</copyright><lastBuildDate>Mon, 24 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ivi.fnwi.uva.nl/aimlab/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>AIM Lab</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/</link>
    </image>
    
    <item>
      <title>Hierarchical Variational Memory for Few-shot Learning Across Domains</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-iclr2022-yingjun/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-iclr2022-yingjun/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Generalize across Domains on Single Test Samples </title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-iclr2022-zehao/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-iclr2022-zehao/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>One paper accepted at NeurIPS 2021.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-nips-jiayi/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-nips-jiayi/</guid>
      <description>&lt;p&gt;Congratulations on Jiayi Shen&amp;rsquo;s paper &amp;ldquo;Variational Multi-Task Learning with Gumbel-Softmax Priors&amp;rdquo; being accepted by NeurIPS 2021.&lt;/p&gt;
&lt;p&gt;Multi-task learning aims to explore task relatedness to improve individual tasks, which is of particular signiﬁcance in the challenging scenario that only limited data is available for each task. To tackle this challenge, we propose variational multi-task learning (VMTL), a general probabilistic inference framework in which we cast multi-task learning as a variational Bayesian inference problem. In this way, task relatedness can be explored in a uniﬁed manner by specifying priors. To leverage the knowledge shared among tasks, we design the prior of a task to be a learnable mixture of the variational posteriors of other tasks, which is learned by the Gumbel-Softmax technique. In contrast to previous methods, our VMTL can exploit task relatedness for both representations and classiﬁers in a single uniﬁed framework by jointly inferring their posteriors. This enables individual tasks to fully leverage inductive biases provided by related tasks, therefore improving the overall performance of all tasks. Experimental results demonstrate that the proposed VMTL is able to effectively tackle a variety of challenging multi-task learning problems with limited training data for both classiﬁcation and regression. Our method consistently surpasses previous methods, including strong Bayesian approaches, and achieves state-of-the-art performance on ﬁve benchmark datasets.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Variational Multi-Task Learning with Gumbel-Softmax Priors</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-nips2021-jiayi/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-nips2021-jiayi/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Artificial Intelligence for Medical Imaging Lab.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-acl-yingjun/</link>
      <pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-acl-yingjun/</guid>
      <description>&lt;p&gt;The AIM Lab opened on 31 January 2020. An update by Assistant Professor Xiantong Zhen at the AIM Lab. AIM Lab is a collaborative initiative of the Inception Institute of Artificial Intelligence from the United Arab Emirates and the University of Amsterdam.&lt;/p&gt;
&lt;p&gt;The research lab is focused on medical image analysis by machine learning, covering active scientific topics of broad interest, including both methods and applications. These topics range from low-level vision and data pre-processing tasks to high-level image/video analysis tasks. From a technical perspective, researchers at the AIM Lab will be working on fundamental and relatively general deep-learning models and algorithms, which will be applied to specific diseases, including but not limited to Alzheimer’s disease, cancer and cardiovascular diseases.
In particular, the AIM Lab will be focusing on the following seven projects in its first five years:&lt;/p&gt;
&lt;p&gt;• Learning with limited data and its applications to medical image analysis&lt;/p&gt;
&lt;p&gt;• Multi-task learning for medical image analysis and data mining&lt;/p&gt;
&lt;p&gt;• Continual learning with its applications to medical image classification&lt;/p&gt;
&lt;p&gt;• Out-of-distribution generalization for medical image analysis&lt;/p&gt;
&lt;p&gt;• Jointly learning from medical images and health records&lt;/p&gt;
&lt;p&gt;• Automated report generation from radiology images.&lt;/p&gt;
&lt;p&gt;The detailed news could be found in: &lt;a href=&#34;https://ivi.uva.nl/content/news/2021/07/artificial-intelligence-for-medical-imaging-lab.html&#34;&gt;https://ivi.uva.nl/content/news/2021/07/artificial-intelligence-for-medical-imaging-lab.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adding ears to a computer counting the bounces on a trampoline.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-cvpr2-yunhua/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-cvpr2-yunhua/</guid>
      <description>&lt;p&gt;Recently, an article from researchers from the Informatics Institute of the University of Amsterdam has been published at the IEEE Conference on Computer Vision and Pattern Recognition; the leading peer-reviewed publication venue in the field of artificial intelligence. The researchers introduced a method for counting repetitions, which are relevant when analyzing human activity (sports), animal behavior (a bee’s waggle dance) or natural phenomena (leaves in the wind) by integrating for the first time the audio modality in a visual counting system based on neural networks.&lt;/p&gt;
&lt;p&gt;Yunhua Zhang and Cees Snoek of the Video &amp;amp; Image Sense Lab (VIS), in collaboration with Ling Shao from the Inception Institute of Artificial Intelligence (IIAI), developed a method for estimating how many times a certain repetitive phenomenon, such as bouncing on a trampoline, slicing an onion, or playing ping pong, happens in a video stream. Their methodology is applicable to any scenario in which repetitive motion patterns exist. By using both sight and sound, as well as their cross-modal interaction, counting predictions are shown to be much more robust than a sight-only model. The results highlight the benefits brought by the use of sound as an addition to sight, especially in harsh vision conditions, for example during low illumination, or when camera viewpoint changes, and even occlusions, where the combination of both sight and sound always outperforms the use of sight only.&lt;/p&gt;
&lt;p&gt;The detailed information could be found in &lt;a href=&#34;https://www.uva.nl/en/shared-content/subsites/informatics-institute/en/news/2021/06/adding-ears-to-a-computer-counting.html&#34;&gt;https://www.uva.nl/en/shared-content/subsites/informatics-institute/en/news/2021/06/adding-ears-to-a-computer-counting.html&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Example Event</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/event/first-dinner/</link>
      <pubDate>Fri, 11 Jun 2021 17:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/event/first-dinner/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Event</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/event/first-offline-aim-lab-gathering/</link>
      <pubDate>Fri, 11 Jun 2021 17:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/event/first-offline-aim-lab-gathering/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper accepted at ICML 2021.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-icml-mohanmad/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-icml-mohanmad/</guid>
      <description>&lt;p&gt;Congratulations on Mohammad Derakhshani&amp;rsquo;s paper &amp;ldquo;Kernel Continual Learning&amp;rdquo; being accepted by ICML 2021.&lt;/p&gt;
&lt;p&gt;This paper introduces kernel continual learning, a simple but effective variant of continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. We deploy an episodic memory unit that stores a subset of samples for each task to learn task-speciﬁc classiﬁers based on kernel ridge regression. This does not require memory replay and systematically avoids task interference in the classiﬁers. We further introduce variational random features to learn a data-driven kernel for each task. To do so, we formulate kernel continual learning as a variational inference problem, where a random Fourier basis is incorporated as the latent variable. The variational posterior distribution over the random Fourier basis is inferred from the coreset of each task. In this way, we are able to generate more informative kernels speciﬁc to each task, and, more importantly, the coreset size can be reduced to achieve more compact memory, resulting in more efﬁcient continual learning based on episodic memory. Extensive evaluation on four benchmarks demonstrates the effectiveness and promise of kernels for continual learning.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>One paper accepted at ICML 2021.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-icml-zehao/</link>
      <pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-icml-zehao/</guid>
      <description>&lt;p&gt;Congratulations on Zehao Xiao&amp;rsquo;s paper &amp;ldquo;A Bit More Bayesian: Domain-Invariant Learning with Uncertainty&amp;rdquo; being accepted by ICML 2021.&lt;/p&gt;
&lt;p&gt;The ICML 2021 paper “A Bit More Bayesian: Domain-Invariant Learning with Uncertaintyce” by Zehao Xiao, Jiayi Shen, Xiantong Zhen, Ling Shao and Cees Snoek is now available. Domain generalization is challenging due to the domain shift and the uncertainty caused by the inaccessibility of target domain data. In this paper, we address both challenges with a probabilistic framework based on variational Bayesian inference, by incorporating uncertainty into neural network weights. We couple domain invariance in a probabilistic formula with the variational Bayesian inference. This enables us to explore domain-invariant learning in a principled way. Specifically, we derive domain-invariant representations and classifiers, which are jointly established in a two-layer Bayesian neural network. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies validate the synergistic benefits of our Bayesian treatment when jointly learning domain-invariant representations and classifiers for domain generalization. Further, our method consistently delivers state-of-the-art mean accuracy on all benchmarks.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>One paper accepted at MICCAI 2021.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-miccai-ivona/</link>
      <pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-miccai-ivona/</guid>
      <description>&lt;p&gt;Congratulations on Ivona Najdenkoska&amp;rsquo;s paper &amp;ldquo;Variational Topic Inference for Chest X-Ray Report Generation&amp;rdquo; being accepted by MICCAI 2021.&lt;/p&gt;
&lt;p&gt;Automating report generation for medical imaging promises to alleviate workload and assist diagnosis in clinical practice. Recent work has shown that deep learning models can successfully caption natural images. However, learning from medical data is challenging due to the diversity and uncertainty inherent in the reports written by diﬀerent radiologists with discrepant expertise and experience. To tackle these challenges, we propose variational topic inference for automatic report generation. Speciﬁcally, we introduce a set of topics as latent variables to guide sentence generation by aligning image and language modalities in a latent space. The topics are inferred in a conditional variational inference framework, with each topic governing the generation of a sentence in the report. Further, we adopt a visual attention module that enables the model to attend to diﬀerent local regions in the image to generate more informative descriptions. We conduct extensive experiments on two benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results demonstrate that our proposed variational topic inference model can generate reports which are not mere copies of reports used in training, while still achieving comparable performance to state-of-the-art methods in terms of standard language generation criteria.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>One paper won the MICCAI Student Travel Awards at MICCAI 2021.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-travel-award-ivona/</link>
      <pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-travel-award-ivona/</guid>
      <description>&lt;p&gt;Congratulations on Ivona Najdenkoska&amp;rsquo;s paper &amp;ldquo;Variational Topic Inference for Chest X-Ray Report Generation&amp;rdquo; won the MICCAI Student Travel Awards at MICCAI 2021.&lt;/p&gt;
&lt;p&gt;The MICCAI Student Travel Awards scheme serves two main functions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To reward the best (such as highest scoring) first author students and to subsidise their attendance to the present their work at the annual MICCAI conference.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To assist in building the MICCAI community by selecting early career participants from countries of lower-income countries from where fewer papers are received and the authors are typically not well funded to attend international meetings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The detailed information could be found in the &lt;a href=&#34;http://miccai.org/about-miccai/awards/student-travel-awards/&#34;&gt;http://miccai.org/about-miccai/awards/student-travel-awards/&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-acl2021-yingjun/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-acl2021-yingjun/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variational Topic Inference for Chest X-Ray Report Generation</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-miccai2021-ivona/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-miccai2021-ivona/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Bit More Bayesian: Domain-Invariant Learning with Uncertainty</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2021-zehao/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2021-zehao/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kernel Continual Learning</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2021-mohammad/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2021-mohammad/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>One paper accepted at CVPR 2021.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-cvpr-yunhua/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-cvpr-yunhua/</guid>
      <description>&lt;p&gt;Congratulations on Yunhua Zhang&amp;rsquo;s paper &amp;ldquo;Repetitive Activity Counting by Sight and Sound&amp;rdquo; being accepted by CVPR 2021.&lt;/p&gt;
&lt;p&gt;This paper strives for repetitive activity counting in videos. Different from existing works, which all analyze the visual video content only, we incorporate for the first time the corresponding sound into the repetition counting process. This benefits accuracy in challenging vision conditions such as occlusion, dramatic camera view changes, low resolution, etc. We propose a model that starts with analyzing the sight and sound streams separately. Then an audiovisual temporal stride decision module and a reliability estimation module are introduced to exploit cross-modal temporal interaction. For learning and evaluation, an existing dataset is repurposed and reorganized to allow for repetition counting with sight and sound. We also introduce a variant of this dataset for repetition counting under challenging vision conditions. Experiments demonstrate the benefit of sound, as well as the other introduced modules, for repetition counting. Our sight-only model already outperforms the state-of-the-art by itself, when we add sound, results improve notably, especially under harsh vision conditions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MetaNorm: Learning to Normalize Few-Shot Batches Across Domains</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-iclr2021-yingjun/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-iclr2021-yingjun/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Repetitive Activity Counting by Sight and Sound</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-cvpr2021-yunhua/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-cvpr2021-yunhua/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variational Prototype Inference for Few-Shot Semantic Segmentation</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-wacv2021-haochen/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-wacv2021-haochen/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>One paper accepted at IPMI 2021.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-ipmi-tom/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-ipmi-tom/</guid>
      <description>&lt;p&gt;Congratulations on Tom van Sonsbeek&amp;rsquo;s paper &amp;ldquo;Variational Knowledge Distillation for Disease Classification in Chest X-Rays&amp;rdquo; being accepted by IPMI 2021.&lt;/p&gt;
&lt;p&gt;Disease classification relying solely on imaging data attracts great interest in medical image analysis. Current models could be fur- ther improved, however, by also employing Electronic Health Records (EHRs), which contain rich information on patients and findings from clinicians. It is challenging to incorporate this information into disease classification due to the high reliance on clinician input in EHRs, lim- iting the possibility for automated diagnosis. In this paper, we propose variational knowledge distillation (VKD), which is a new probabilistic inference framework for disease classification based on X-rays that lever- ages knowledge from EHRs. Specifically, we introduce a conditional la- tent variable model, where we infer the latent representation of the X-ray image with the variational posterior conditioning on the associated EHR text. By doing so, the model acquires the ability to extract the visual features relevant to the disease during learning and can therefore perform more accurate classification for unseen patients at inference based solely on their X-ray scans. We demonstrate the effectiveness of our method on three public benchmark datasets with paired X-ray images and EHRs. The results show that the proposed variational knowledge distillation can consistently improve the performance of medical image classification and significantly surpasses current methods.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Variational Knowledge Distillation for Disease Classification in Chest X-Rays</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-ipmi2021-tom/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-ipmi2021-tom/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>One paper accepted at ICLR 2021.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2021-iclr-yingjun/</link>
      <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2021-iclr-yingjun/</guid>
      <description>&lt;p&gt;Congratulations on Yingjun Du&amp;rsquo;s paper &amp;ldquo;MetaNorm: Learning to Normalize Few-Shot Batches Across Domains&amp;rdquo; being accepted by ICLR 2021.&lt;/p&gt;
&lt;p&gt;Batch normalization plays a crucial role when training deep neural networks. However, batch statistics become unstable with small batch sizes and are unreliable in the presence of distribution shifts. We propose MetaNorm, a simple yet effective meta-learning normalization. It tackles the aforementioned issues in a uniﬁed way by leveraging the meta-learning setting and learns to infer adaptive statistics for batch normalization. MetaNorm is generic, ﬂexible and model-agnostic, making it a simple plug-and-play module that is seamlessly embedded into existing meta-learning approaches. It can be efﬁciently implemented by lightweight hypernetworks with low computational cost. We verify its effectiveness by extensive evaluation on representative tasks suffering from the small batch and domain shift problems: few-shot learning and domain generalization. We further introduce an even more challenging setting: few-shot domain generalization. Results demonstrate that MetaNorm consistently achieves better, or at least competitive, accuracy compared to existing batch normalization methods.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>One paper accepted at NeurIPS 2020.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2020-nips-xiantong/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2020-nips-xiantong/</guid>
      <description>&lt;p&gt;Congratulations on Xiantong Zhen&amp;rsquo;s paper &amp;ldquo;Learning to Learn Variational Semantic Memory&amp;rdquo; being published by NeurIPS 2020.&lt;/p&gt;
&lt;p&gt;In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efﬁciently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the beneﬁt of variational semantic memory in boosting few-shot recognition.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>One paper accepted at ECCV 2020.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2020-eccv-yingjun/</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2020-eccv-yingjun/</guid>
      <description>&lt;p&gt;Congratulations on Yingjun Du&amp;rsquo;s paper &amp;ldquo;Learning to Learn with Variational Information Bottleneck for Domain Generalization&amp;rdquo; being accepted by ICML 2020.&lt;/p&gt;
&lt;p&gt;Domain generalization models learn to generalize to previously unseen domains, but suffer from prediction uncertainty and domain shift. In this paper, we address both problems. We introduce a probabilistic meta-learning model for domain generalization, in which classifier parameters shared across domains are modeled as distributions. This enables better handling of prediction uncertainty on unseen domains. To deal with domain shift, we learn domain-invariant representations by the proposed principle of meta variational information bottleneck, we call MetaVIB. MetaVIB is derived from novel variational bounds of mutual information, by leveraging the meta-learning setting of domain generalization. Through episodic training, MetaVIB learns to gradually narrow domain gaps to establish domain-invariant representations, while simultaneously maximizing prediction accuracy. We conduct experiments on three benchmarks for cross-domain visual recognition. Comprehensive ablation studies validate the benefits of MetaVIB for domain generalization. The comparison results demonstrate our method outperforms previous approaches consistently.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Few-Shot Semantic Segmentation with Democratic Attention Networks</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-eccv2020-xiantong/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-eccv2020-xiantong/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Learn with Variational Information Bottleneck for Domain Generalization</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-eccv2020-yingjun/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-eccv2020-yingjun/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Few-Shot Ensemble Learning for Video Classification with SlowFast Memory Networks</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-acm2020-xiantong/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-acm2020-xiantong/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Learn Variational Semantic Memory</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-nips2020-xiantong/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-nips2020-xiantong/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>One paper accepted at ICML 2020.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2020-icml-xiantong/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2020-icml-xiantong/</guid>
      <description>&lt;p&gt;Congratulations on Xiantong Zhen&amp;rsquo;s paper &amp;ldquo;Learning to Learn Kernels with Variational Random Features&amp;rdquo; being accepted by ICML 2020.&lt;/p&gt;
&lt;p&gt;We introduce kernels with random Fourier features in the meta-learning framework for few-shot learning. We propose meta variational random features (MetaVRF) to learn adaptive kernels for the base-learner, which is developed in a latent variable model by treating the random feature basis as the latent variable. We formulate the optimization of MetaVRF as a variational inference problem by deriving an evidence lower bound under the meta-learning framework. To incorporate shared knowledge from related tasks, we propose a context inference of the posterior, which is established by an LSTM architecture. The LSTMbased inference network effectively integrates the context information of previous tasks with taskspecific information, generating informative and adaptive features. The learned MetaVRF is able to produce kernels of high representational power with a relatively low spectral sampling rate and also enables fast adaptation to new tasks. Experimental results on a variety of few-shot regression and classification tasks demonstrate that MetaVRF can deliver much better, or at least competitive, performance compared to existing metalearning alternatives.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pixel-level Non-local Image Smoothing with Objective Evaluation</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/journal-acm2020-xiantong/</link>
      <pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/journal-acm2020-xiantong/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conditional Variational Image Deraining</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/journal-tip2020-yingjun/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/journal-tip2020-yingjun/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Learn Kernels with Variational Random Features</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2020-xiantong/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2020-xiantong/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ivi.fnwi.uva.nl/aimlab/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://ivi.fnwi.uva.nl/aimlab/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper accepted at ICLR 2022.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2022-iclr-yingjun/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2022-iclr-yingjun/</guid>
      <description>&lt;p&gt;Congratulations on Yingjun Du&amp;rsquo;s paper &amp;ldquo;Hierarchical Variational Memory for Few-shot Learning Across Domains&amp;rdquo; being accepted by ICLR 2022.&lt;/p&gt;
&lt;p&gt;Neural memory enables fast adaptation to new tasks with just a few training samples. Existing memory models store features only from the single last layer, which does not generalize well in presence of a domain shift between training and test distributions. Rather than relying on a flat memory, we propose a hierarchical alternative that stores features at different semantic levels. We introduce a hierarchical prototype model, where each level of the prototype fetches corresponding information from the hierarchical memory. The model is endowed with the ability to flexibly rely on features at different semantic levels if the domain shift circumstances so demand. We meta-learn the model by a newly derived hierarchical variational inference framework, where hierarchical memory and prototypes are jointly optimized. To explore and exploit the importance of different semantic levels, we further propose to learn the weights associated with the prototype at each level in a data-driven way, which enables the model to adaptively choose the most generalizable features. We conduct thorough ablation studies to demonstrate the effectiveness of each component in our model. The new state-of-the-art performance on cross-domain and competitive performance on traditional few-shot classification further substantiates the benefit of hierarchical variational memory.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>One paper accepted at ICLR 2022.</title>
      <link>https://ivi.fnwi.uva.nl/aimlab/post/2022-iclr-zehao/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ivi.fnwi.uva.nl/aimlab/post/2022-iclr-zehao/</guid>
      <description>&lt;p&gt;Congratulations on Zehao Xiao&amp;rsquo;s paper &amp;ldquo;Learning to Generalize across Domains on Single Test Samples &amp;quot; being accepted by ICLR 2022.&lt;/p&gt;
&lt;p&gt;We strive to learn a model from a set of source domains that generalizes well to unseen target domains. The main challenge in such a domain generalization scenario is the unavailability of any target domain data during training, resulting in the learned model not being explicitly adapted to the unseen target domains. We propose learning to generalize across domains on single test samples. We leverage a meta-learning paradigm to learn our model to acquire the ability of adaptation with single samples at training time so as to further adapt itself to each single test sample at test time. We formulate the adaptation to the single test sample as a variational Bayesian inference problem, which incorporates the test sample as a conditional into the generation of model parameters. The adaptation to each test sample requires only one feed-forward computation at test time without any fine-tuning or self-supervised training on additional data from the unseen domains. We conduct extensive ablation studies to demonstrate the effectiveness and advantages. Further, our model achieves at least comparable &amp;ndash; and often better &amp;ndash; performance than state-of-the-art methods on multiple benchmarks for domain generalization.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
