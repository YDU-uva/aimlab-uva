[{"authors":null,"categories":null,"content":"Cees G.M. Snoek is a full professor in computer science at the University of Amsterdam, where he heads the Video and Image Sense Lab. He is also a director of three public-private AI research labs: QUVA Lab with Qualcomm, Atlas Lab with TomTom and AIM Lab with the Inception Institute of Artificial Intelligence. At University spin-off Kepler Vision Technologies he acts as Chief Scientific Officer. Professor Snoek is also the director of the master program in Artificial Intelligence and co-founder of the Innovation Center for Artificial Intelligence.\nProfessor Snoek is the lead researcher of the award-winning MediaMill Semantic Video Search Engine, which was the most consistent top performer in the yearly NIST TRECVID evaluations for over a decade. He was general chair of ACM Multimedia 2016 in Amsterdam, program chair for ICMR 2017, and initiator of the VideOlympics 2007-2009. He is a lecturer of post-doctoral courses and tutorials given at international conferences and European summer schools. He is a senior member of IEEE and ACM. Cees is recipient of an NWO Veni award, a Fulbright Junior Scholarship, an NWO Vidi award, and the Netherlands Prize for ICT Research. All for research excellence. Several of his Ph.D. students and Post-docs have won awards, including the IEEE Transactions on Multimedia Prize Paper Award the SIGMM Best Ph.D. Thesis Award, the Best Paper Award of ACM Multimedia, an NWO Veni award and the Best Paper Award of ACM Multimedia Retrieval. Five of his former mentees serve as assistant and associate professors.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3a4e8027a6100a47d097244b3475093d","permalink":"/author/cees-g.m.-snoek/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cees-g.m.-snoek/","section":"authors","summary":"Cees G.M. Snoek is a full professor in computer science at the University of Amsterdam, where he heads the Video and Image Sense Lab. He is also a director of three public-private AI research labs: QUVA Lab with Qualcomm, Atlas Lab with TomTom and AIM Lab with the Inception Institute of Artificial Intelligence.","tags":null,"title":"Cees G.M. Snoek","type":"authors"},{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4f6aa83981dfb2e9ee783294522983de","permalink":"/author/ivona-najdenkoska/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ivona-najdenkoska/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Ivona Najdenkoska","type":"authors"},{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0ffa1c5eb3e1e708146516eb9219843d","permalink":"/author/jiayi-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiayi-shen/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Jiayi Shen","type":"authors"},{"authors":null,"categories":null,"content":"Professor Ling Shao is Chair in Computer Vision and Head of the Computer Vision and Artificial Intelligence Group with the Department of Computer Science and Digital Technologies at Northumbria University, Newcastle upon Tyne and an Advanced Visiting Fellow with the Department of Electronic and Electrical Engineering at the University of Sheffield. He received the B.Eng. degree in Electronic and Information Engineering from the University of Science and Technology of China (USTC), the M.Sc. degree in Medical Image Analysis and the Ph.D. (D.Phil.) degree in Computer Vision at the Robotics Research Group from the University of Oxford. Previously, he was a Senior Lecturer (2009-2014) with the Department of Electronic and Electrical Engineering at the University of Sheffieldand a Senior Scientist (2005-2009) with Philips Research, The Netherlands. His research interests include Computer Vision, Image/Video Processing, Pattern Recognition and Machine Learning. He has authored/co-authored over 200 papers in refereed journals/conferences such as IEEE TPAMI, TIP, TNNLS, IJCV, ICCV, CVPR, IJCAI and ACM MM, and holds over 10 EU/US patents.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a2c86fc6ee81f38fc176950fa26979ea","permalink":"/author/ling-shao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ling-shao/","section":"authors","summary":"Professor Ling Shao is Chair in Computer Vision and Head of the Computer Vision and Artificial Intelligence Group with the Department of Computer Science and Digital Technologies at Northumbria University, Newcastle upon Tyne and an Advanced Visiting Fellow with the Department of Electronic and Electrical Engineering at the University of Sheffield.","tags":null,"title":"Ling Shao","type":"authors"},{"authors":null,"categories":null,"content":"I did my masters at the Free University Amsterdam in Computer Science with a specialization in medical computer science. During my PhD work I focussed on image analysis both from a theoretical point of view, establishing the limits in accuracy one can achieve when measuring shape on a digital grid, to more applied shape analysis in biological and medical images. Part of this research was on deformable shape models and performed at Yale University. From there I moved to document and video analysis, spending four months in San Diego studying film theory and how it can help automatic video analysis, and subsequently more and more into the development of methods for accessing large image and video collections by their content. In such a setting, next to image analysis and understanding, user interaction is a crucial element and with it comes the need for advanced visualizations of the collection and the results of user queries. Currently we are taking this a step further into multimedia analytics being the integration of multimedia analysis, multimedia mining, information visualization, and multimedia interaction into a coherent framework yielding more than its constituent components.\nMuch of the research has been done in a multi-disciplinary setting, with applications in biology, medicine, broadcasting, forensics, urban analytics, and cultural heritage.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68f32b9e611357fd641418138a32ae1d","permalink":"/author/marcel-worring/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/marcel-worring/","section":"authors","summary":"I did my masters at the Free University Amsterdam in Computer Science with a specialization in medical computer science. During my PhD work I focussed on image analysis both from a theoretical point of view, establishing the limits in accuracy one can achieve when measuring shape on a digital grid, to more applied shape analysis in biological and medical images.","tags":null,"title":"Marcel Worring","type":"authors"},{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68ff9ddf10c95b9f79d2247cf7bb16a3","permalink":"/author/mohammad-derakhshani/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mohammad-derakhshani/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Mohammad Derakhshani","type":"authors"},{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f541de4c050c81660411f12acb94a15f","permalink":"/author/tom-van-sonsbeek/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tom-van-sonsbeek/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Tom van Sonsbeek","type":"authors"},{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7bb23775877a3674a6e5f68e247eec2f","permalink":"/author/xiantong-zhen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiantong-zhen/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Xiantong Zhen","type":"authors"},{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a6ad75b4889ca16c7395b21531e716f","permalink":"/author/yingjun-du/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yingjun-du/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Yingjun Du","type":"authors"},{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ab672aaec854c166be615851dbcb76ea","permalink":"/author/yunhua-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yunhua-zhang/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Yunhua Zhang","type":"authors"},{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c3357e3eb2bca46a8f94f2d3b0dcb8ae","permalink":"/author/zehao-xiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zehao-xiao/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Zehao Xiao","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["Xiantong Zhen","Yingjun Du","Huan Xiong","Cees G.M. Snoek","Ling Shao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"ca0b33950b8e355d871b735cc4ae8d18","permalink":"/publication/conference-nips2020-xiantong/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/conference-nips2020-xiantong/","section":"publication","summary":"In this work, we introduce kernels with random Fourier features in the meta-learning framework to leverage their strong few-shot learning ability.","tags":["Source Themes"],"title":"Learning to Learn Variational Semantic Memory","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations on Yunhua Zhang\u0026rsquo;s paper \u0026ldquo;Repetitive Activity Counting by Sight and Sound\u0026rdquo; being accepted by CVPR 2021.\nThis paper strives for repetitive activity counting in videos. Different from existing works, which all analyze the visual video content only, we incorporate for the first time the corresponding sound into the repetition counting process. This benefits accuracy in challenging vision conditions such as occlusion, dramatic camera view changes, low resolution, etc. We propose a model that starts with analyzing the sight and sound streams separately. Then an audiovisual temporal stride decision module and a reliability estimation module are introduced to exploit cross-modal temporal interaction. For learning and evaluation, an existing dataset is repurposed and reorganized to allow for repetition counting with sight and sound. We also introduce a variant of this dataset for repetition counting under challenging vision conditions. Experiments demonstrate the benefit of sound, as well as the other introduced modules, for repetition counting. Our sight-only model already outperforms the state-of-the-art by itself, when we add sound, results improve notably, especially under harsh vision conditions.\n","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"f04c95b19cde36c2541ff24813312ea0","permalink":"/post/2021-cvpr-yunhua/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/post/2021-cvpr-yunhua/","section":"post","summary":"Congratulations on Yunhua Zhang\u0026rsquo;s paper \u0026ldquo;Repetitive Activity Counting by Sight and Sound\u0026rdquo; being accepted by CVPR 2021.\n","tags":null,"title":"One paper accepted at CVPR 2021.","type":"post"},{"authors":["Yunhua Zhang","Ling Shao","Cees G.M. Snoek",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"34bbeb3cae70c5c77acb57e3bb405dd3","permalink":"/publication/conference-cvpr2021-yunhua/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/conference-cvpr2021-yunhua/","section":"publication","summary":"This paper strives for repetitive activity counting in videos. Different from existing works, which all analyze the visual video content only, we incorporate for the first time the corresponding sound into the repetition counting process.","tags":["Source Themes"],"title":"Repetitive Activity Counting by Sight and Sound","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations on Tom van Sonsbeek\u0026rsquo;s paper \u0026ldquo;Variational Knowledge Distillation for Disease Classification in Chest X-Rays\u0026rdquo; being accepted by IPMI 2021.\nDisease classification relying solely on imaging data attracts great interest in medical image analysis. Current models could be fur- ther improved, however, by also employing Electronic Health Records (EHRs), which contain rich information on patients and findings from clinicians. It is challenging to incorporate this information into disease classification due to the high reliance on clinician input in EHRs, lim- iting the possibility for automated diagnosis. In this paper, we propose variational knowledge distillation (VKD), which is a new probabilistic inference framework for disease classification based on X-rays that lever- ages knowledge from EHRs. Specifically, we introduce a conditional la- tent variable model, where we infer the latent representation of the X-ray image with the variational posterior conditioning on the associated EHR text. By doing so, the model acquires the ability to extract the visual features relevant to the disease during learning and can therefore perform more accurate classification for unseen patients at inference based solely on their X-ray scans. We demonstrate the effectiveness of our method on three public benchmark datasets with paired X-ray images and EHRs. The results show that the proposed variational knowledge distillation can consistently improve the performance of medical image classification and significantly surpasses current methods.\n","date":1612224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612224000,"objectID":"00f4227cd76c47cd51c2a9262bbf7102","permalink":"/post/2021-ipmi-tom/","publishdate":"2021-02-02T00:00:00Z","relpermalink":"/post/2021-ipmi-tom/","section":"post","summary":"Congratulations on Tom van Sonsbeek\u0026rsquo;s paper \u0026ldquo;Variational Knowledge Distillation for Disease Classification in Chest X-Rays\u0026rdquo; being accepted by IPMI 2021.\n","tags":null,"title":"One paper accepted at IPMI 2021.","type":"post"},{"authors":["Tom van Sonsbeek","Xiantong Zhen","Marcel Worring","Ling Shao",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"e8ec1dd87fe9dd9847fc2f8e63cb9a07","permalink":"/publication/conference-ipmi2021-tom/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/publication/conference-ipmi2021-tom/","section":"publication","summary":"We propose variational knowledge distillation (VKD), which is a new probabilistic inference framework for disease classification based on X-rays that lever- ages knowledge from EHRs.","tags":["Source Themes"],"title":"Variational Knowledge Distillation for Disease Classification in Chest X-Rays","type":"publication"},{"authors":["Zehao Xiao","Jiayin Shen","Xiantong Zhen","Ling Shao","Cees G.M. Snoek"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"f89028a5f4942597bde9dd61092ed44a","permalink":"/publication/preprint-icml2021-zehao/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/preprint-icml2021-zehao/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"Variational Invariant Learning for Bayesian Domain Generalization","type":"publication"},{"authors":["Jiayi Shen","Xiantong Zhen","Marcel Worring","Ling Shao"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"966ae310362ddca40473b3fbc2d71df4","permalink":"/publication/preprint-icml2021-jiayi/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/preprint-icml2021-jiayi/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"Variational Multi-Task Learning","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations on Yingjun Du\u0026rsquo;s paper \u0026ldquo;MetaNorm: Learning to Normalize Few-Shot Batches Across Domains\u0026rdquo; being accepted by ICLR 2021.\nBatch normalization plays a crucial role when training deep neural networks. However, batch statistics become unstable with small batch sizes and are unreliable in the presence of distribution shifts. We propose MetaNorm, a simple yet effective meta-learning normalization. It tackles the aforementioned issues in a uniﬁed way by leveraging the meta-learning setting and learns to infer adaptive statistics for batch normalization. MetaNorm is generic, ﬂexible and model-agnostic, making it a simple plug-and-play module that is seamlessly embedded into existing meta-learning approaches. It can be efﬁciently implemented by lightweight hypernetworks with low computational cost. We verify its effectiveness by extensive evaluation on representative tasks suffering from the small batch and domain shift problems: few-shot learning and domain generalization. We further introduce an even more challenging setting: few-shot domain generalization. Results demonstrate that MetaNorm consistently achieves better, or at least competitive, accuracy compared to existing batch normalization methods.\n","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"7fe506da260dc148e48c2f7151f234b4","permalink":"/post/2021-iclr-yingjun/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/post/2021-iclr-yingjun/","section":"post","summary":"Congratulations on Yingjun Du\u0026rsquo;s paper \u0026ldquo;MetaNorm: Learning to Normalize Few-Shot Batches Across Domains\u0026rdquo; being accepted by ICLR 2021.\n","tags":null,"title":"One paper accepted at ICLR 2021.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations on Xiantong Zhen\u0026rsquo;s paper \u0026ldquo;Learning to Learn Variational Semantic Memory\u0026rdquo; being published by NeurIPS 2020.\nIn this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efﬁciently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the beneﬁt of variational semantic memory in boosting few-shot recognition.\n","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"4f08b1e49276f51cb9836c41f69e7394","permalink":"/post/2020-nips-xiantong/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/post/2020-nips-xiantong/","section":"post","summary":"Congratulations on Xiantong Zhen\u0026rsquo;s paper \u0026ldquo;Learning to Learn Variational Semantic Memory\u0026rdquo; being published by NeurIPS 2020.\n","tags":null,"title":"One paper accepted at NeurIPS 2020.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations on Yingjun Du\u0026rsquo;s paper \u0026ldquo;Learning to Learn with Variational Information Bottleneck for Domain Generalization\u0026rdquo; being accepted by ICML 2020.\nDomain generalization models learn to generalize to previously unseen domains, but suffer from prediction uncertainty and domain shift. In this paper, we address both problems. We introduce a probabilistic meta-learning model for domain generalization, in which classifier parameters shared across domains are modeled as distributions. This enables better handling of prediction uncertainty on unseen domains. To deal with domain shift, we learn domain-invariant representations by the proposed principle of meta variational information bottleneck, we call MetaVIB. MetaVIB is derived from novel variational bounds of mutual information, by leveraging the meta-learning setting of domain generalization. Through episodic training, MetaVIB learns to gradually narrow domain gaps to establish domain-invariant representations, while simultaneously maximizing prediction accuracy. We conduct experiments on three benchmarks for cross-domain visual recognition. Comprehensive ablation studies validate the benefits of MetaVIB for domain generalization. The comparison results demonstrate our method outperforms previous approaches consistently.\n","date":1600128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600128000,"objectID":"0e3d9724544c799ac5d038b7ac1b0ddf","permalink":"/post/2020-eccv-yingjun/","publishdate":"2020-09-15T00:00:00Z","relpermalink":"/post/2020-eccv-yingjun/","section":"post","summary":"Congratulations on Yingjun Du\u0026rsquo;s paper \u0026ldquo;Learning to Learn with Variational Information Bottleneck for Domain Generalization\u0026rdquo; being accepted by ICML 2020.\n","tags":null,"title":"One paper accepted at ECCV 2020.","type":"post"},{"authors":["Haochen Wang","Xudong Zhang","Yutao Hu","Yandan Yang","Xianbin Cao","Xiantong Zhen",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1599004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599004800,"objectID":"2ef6ca1774247e1776c6cdcddb14d28b","permalink":"/publication/conference-eccv2020-xiantong/","publishdate":"2020-09-02T00:00:00Z","relpermalink":"/publication/conference-eccv2020-xiantong/","section":"publication","summary":"Few-Shot Segmentation, Graph Attention, Democratic Attention Network, Multi-Scale Guidance","tags":["Source Themes"],"title":"Few-Shot Semantic Segmentation with Democratic Attention Networks","type":"publication"},{"authors":["Yingjun Du","Jun Xu","Huan Xiong","Qiang Qiu","Xiantong Zhen","Cees G.M. Snoek","Ling Shao",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"d066d9c8ab04d5eb0c8723d31531fa6b","permalink":"/publication/conference-eccv2020-yingjun/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/conference-eccv2020-yingjun/","section":"publication","summary":"Meta Learning, Domain Generalization, Variational Inference, Information Bottleneck","tags":["Source Themes"],"title":"Learning to Learn with Variational Information Bottleneck for Domain Generalization","type":"publication"},{"authors":["Yingjun Du","Xiantong Zhen","Ling Shao","Cees G.M. Snoek",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"c7b7e5bb066a88fc1327ff11028686a9","permalink":"/publication/conference-iclr2021-yingjun/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/conference-iclr2021-yingjun/","section":"publication","summary":"We propose MetaNorm, a simple yet effective meta-learning normalization approach that learns adaptive statistics for few-shot classification and domain generalization.","tags":["Source Themes"],"title":"MetaNorm: Learning to Normalize Few-Shot Batches Across Domains","type":"publication"},{"authors":["Mengshi Qi","Jie Qin","Xiantong Zhen","Di Huang","Yi Yang","Jiebo Luo",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"eb64e99f8279515fb3620695474d3770","permalink":"/publication/conference-acm2020-xiantong/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/conference-acm2020-xiantong/","section":"publication","summary":"In this paper, we address few-shot video classification by learning an ensemble of SlowFast networks augmented with memory units.","tags":["Source Themes"],"title":"Few-Shot Ensemble Learning for Video Classification with SlowFast Memory Networks","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations on Xiantong Zhen\u0026rsquo;s paper \u0026ldquo;Learning to Learn Kernels with Variational Random Features\u0026rdquo; being accepted by ICML 2020.\nWe introduce kernels with random Fourier features in the meta-learning framework for few-shot learning. We propose meta variational random features (MetaVRF) to learn adaptive kernels for the base-learner, which is developed in a latent variable model by treating the random feature basis as the latent variable. We formulate the optimization of MetaVRF as a variational inference problem by deriving an evidence lower bound under the meta-learning framework. To incorporate shared knowledge from related tasks, we propose a context inference of the posterior, which is established by an LSTM architecture. The LSTMbased inference network effectively integrates the context information of previous tasks with taskspecific information, generating informative and adaptive features. The learned MetaVRF is able to produce kernels of high representational power with a relatively low spectral sampling rate and also enables fast adaptation to new tasks. Experimental results on a variety of few-shot regression and classification tasks demonstrate that MetaVRF can deliver much better, or at least competitive, performance compared to existing metalearning alternatives.\n","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588982400,"objectID":"2c3328b2ebcc0c697a9faae11ee9b46d","permalink":"/post/2020-icml-xiantong/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/post/2020-icml-xiantong/","section":"post","summary":"Congratulations on Xiantong Zhen\u0026rsquo;s paper \u0026ldquo;Learning to Learn Kernels with Variational Random Features\u0026rdquo; being accepted by ICML 2020.\n","tags":null,"title":"One paper accepted at ICML 2020.","type":"post"},{"authors":["Zhiang Liu","Yingkun Hou","Xiantong Zhen","Jun Xu","Ling Shao","MingMing Cheng"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1583193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583193600,"objectID":"84df2f05a823c11ec775566bd6bb6432","permalink":"/publication/journal-acm2020-xiantong/","publishdate":"2020-03-03T00:00:00Z","relpermalink":"/publication/journal-acm2020-xiantong/","section":"publication","summary":"Image smoothing, benchmark dataset, performance evaluation, pixel-level non-local self similarity.","tags":["Source Themes"],"title":"Pixel-level Non-local Image Smoothing with Objective Evaluation","type":"publication"},{"authors":["Yingjun Du","Jun Xu","Xiantong Zhen","MingMing Cheng","Ling Shao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"7e89d6841b4c64cd167235b422971bac","permalink":"/publication/journal-tip2020-yingjun/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/journal-tip2020-yingjun/","section":"publication","summary":"In this paper, we propose a Conditional Variational Image Deraining (CVID) network for better deraining performance, leveraging the exclusive generative ability of Conditional Variational Auto-Encoder (CVAE) on providing diverse predictions for the rainy image.","tags":["Source Themes"],"title":"Conditional Variational Image Deraining","type":"publication"},{"authors":["Xiantong Zhen","Haoliang Sun","Yingjun Du","Jun Xu","Yilong Yin","Ling Shao","Cees G.M. Snoek"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"3cd82760ddef6965efb4743dd565b701","permalink":"/publication/conference-icml2020-xiantong/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/conference-icml2020-xiantong/","section":"publication","summary":"In this work, we introduce kernels with random Fourier features in the meta-learning framework to leverage their strong few-shot learning ability.","tags":["Source Themes"],"title":"Learning to Learn Kernels with Variational Random Features","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]