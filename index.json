[{"authors":null,"categories":null,"content":"Cees G.M. Snoek is a full professor in computer science at the University of Amsterdam, where he heads the Video and Image Sense Lab. He is also a director of three public-private AI research labs: QUVA Lab with Qualcomm, Atlas Lab with TomTom and AIM Lab with the Inception Institute of Artificial Intelligence. At University spin-off Kepler Vision Technologies he acts as Chief Scientific Officer. Professor Snoek is also the director of the master program in Artificial Intelligence and co-founder of the Innovation Center for Artificial Intelligence.\nProfessor Snoek is the lead researcher of the award-winning MediaMill Semantic Video Search Engine, which was the most consistent top performer in the yearly NIST TRECVID evaluations for over a decade. He was general chair of ACM Multimedia 2016 in Amsterdam, program chair for ICMR 2017, and initiator of the VideOlympics 2007-2009. He is a lecturer of post-doctoral courses and tutorials given at international conferences and European summer schools. He is a senior member of IEEE and ACM. Cees is recipient of an NWO Veni award, a Fulbright Junior Scholarship, an NWO Vidi award, and the Netherlands Prize for ICT Research. All for research excellence. Several of his Ph.D. students and Post-docs have won awards, including the IEEE Transactions on Multimedia Prize Paper Award the SIGMM Best Ph.D. Thesis Award, the Best Paper Award of ACM Multimedia, an NWO Veni award and the Best Paper Award of ACM Multimedia Retrieval. Five of his former mentees serve as assistant and associate professors.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3a4e8027a6100a47d097244b3475093d","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/cees-g.m.-snoek/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/cees-g.m.-snoek/","section":"authors","summary":"Cees G.M. Snoek is a full professor in computer science at the University of Amsterdam, where he heads the Video and Image Sense Lab. He is also a director of three public-private AI research labs: QUVA Lab with Qualcomm, Atlas Lab with TomTom and AIM Lab with the Inception Institute of Artificial Intelligence.","tags":null,"title":"Cees G.M. Snoek","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4f6aa83981dfb2e9ee783294522983de","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/ivona-najdenkoska/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/ivona-najdenkoska/","section":"authors","summary":"","tags":null,"title":"Ivona Najdenkoska","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0ffa1c5eb3e1e708146516eb9219843d","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/jiayi-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/jiayi-shen/","section":"authors","summary":"","tags":null,"title":"Jiayi Shen","type":"authors"},{"authors":null,"categories":null,"content":"Ling Shao is the Founding CEO and Chief Scientist of the Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE. He was the initiator of the world\u0026rsquo;s first AI university - Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), for which he served as the Founding Provost and Executive Vice President (2019-2021). He received his B.Eng. degree in Electronic and Information Engineering from the University of Science and Technology of China (USTC), his M.Sc. degree in Medical Image Analysis and his Ph.D. (D.Phil.) degree in Computer Vision from the University of Oxford. Previously, He was Chair Professor (2016-2018) in Computer Vision and Machine Learning with the University of East Anglia, Norwich, UK, Chair Professor (2014-2016) in Computer Vision and Machine Intelligence with Northumbria University, Newcastle upon Tyne, UK, a Senior Lecturer (2009-2014) with the University of Sheffield, UK and a Senior Scientist (2005-2009) with Philips Research, The Netherlands. His research interests include Computer Vision, Deep Learning, Medical Imaging, and Vision and Language. He has authored/co-authored over 400 papers in top conferences/journals such as ICCV, CVPR, ECCV, NeurIPS, ICML, TPAMI and IJCV and over 20 EU/US patents.\nShao served as an editorial board member of IEEE Transactions on Image Processing, IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Circuits and Systems for Video Technology, IEEE Transactions on Cybernetics, and several other journals. He has edited four books and several special issues for journals such as IJCV and Pattern Recognition. He was the General Chair for BMVC 2018, was an Area Chair or Program Committee member for many conferences, including ICCV, CVPR, ECCV and ACM MM, and will be the General Chair for IJCB 2022. He was selected as a \u0026lsquo;Highly Cited Researcher\u0026rsquo; by the Web of Science (Thomson Reuters) in 2018/2019/2020. He is a Fellow of the IEEE, a Fellow of the IAPR, a Fellow of the British Computer Society, a Fellow of the IET, and a Life Member of the ACM. He was elected as a Member of the Mohammed bin Rashid Academy of Scientists (the National Academy of the UAE) in 2021.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a2c86fc6ee81f38fc176950fa26979ea","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/ling-shao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/ling-shao/","section":"authors","summary":"Ling Shao is the Founding CEO and Chief Scientist of the Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE. He was the initiator of the world\u0026rsquo;s first AI university - Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), for which he served as the Founding Provost and Executive Vice President (2019-2021).","tags":null,"title":"Ling Shao","type":"authors"},{"authors":null,"categories":null,"content":"I did my masters at the Free University Amsterdam in Computer Science with a specialization in medical computer science. During my PhD work I focussed on image analysis both from a theoretical point of view, establishing the limits in accuracy one can achieve when measuring shape on a digital grid, to more applied shape analysis in biological and medical images. Part of this research was on deformable shape models and performed at Yale University. From there I moved to document and video analysis, spending four months in San Diego studying film theory and how it can help automatic video analysis, and subsequently more and more into the development of methods for accessing large image and video collections by their content. In such a setting, next to image analysis and understanding, user interaction is a crucial element and with it comes the need for advanced visualizations of the collection and the results of user queries. Currently we are taking this a step further into multimedia analytics being the integration of multimedia analysis, multimedia mining, information visualization, and multimedia interaction into a coherent framework yielding more than its constituent components.\nMuch of the research has been done in a multi-disciplinary setting, with applications in biology, medicine, broadcasting, forensics, urban analytics, and cultural heritage.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68f32b9e611357fd641418138a32ae1d","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/marcel-worring/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/marcel-worring/","section":"authors","summary":"I did my masters at the Free University Amsterdam in Computer Science with a specialization in medical computer science. During my PhD work I focussed on image analysis both from a theoretical point of view, establishing the limits in accuracy one can achieve when measuring shape on a digital grid, to more applied shape analysis in biological and medical images.","tags":null,"title":"Marcel Worring","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68ff9ddf10c95b9f79d2247cf7bb16a3","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/mohammad-derakhshani/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/mohammad-derakhshani/","section":"authors","summary":"","tags":null,"title":"Mohammad Derakhshani","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f541de4c050c81660411f12acb94a15f","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/tom-van-sonsbeek/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/tom-van-sonsbeek/","section":"authors","summary":"","tags":null,"title":"Tom van Sonsbeek","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7bb23775877a3674a6e5f68e247eec2f","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/xiantong-zhen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/xiantong-zhen/","section":"authors","summary":"","tags":null,"title":"Xiantong Zhen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a6ad75b4889ca16c7395b21531e716f","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/yingjun-du/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/yingjun-du/","section":"authors","summary":"","tags":null,"title":"Yingjun Du","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ab672aaec854c166be615851dbcb76ea","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/yunhua-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/yunhua-zhang/","section":"authors","summary":"","tags":null,"title":"Yunhua Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c3357e3eb2bca46a8f94f2d3b0dcb8ae","permalink":"https://ivi.fnwi.uva.nl/aimlab/author/zehao-xiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/author/zehao-xiao/","section":"authors","summary":"","tags":null,"title":"Zehao Xiao","type":"authors"},{"authors":null,"categories":null,"content":"Congratulations on Jiayi Shen\u0026rsquo;s paper \u0026ldquo;Variational Multi-Task Learning with Gumbel-Softmax Priors\u0026rdquo; being accepted by NeurIPS 2021.\nMulti-task learning aims to explore task relatedness to improve individual tasks, which is of particular signiﬁcance in the challenging scenario that only limited data is available for each task. To tackle this challenge, we propose variational multi-task learning (VMTL), a general probabilistic inference framework in which we cast multi-task learning as a variational Bayesian inference problem. In this way, task relatedness can be explored in a uniﬁed manner by specifying priors. To leverage the knowledge shared among tasks, we design the prior of a task to be a learnable mixture of the variational posteriors of other tasks, which is learned by the Gumbel-Softmax technique. In contrast to previous methods, our VMTL can exploit task relatedness for both representations and classiﬁers in a single uniﬁed framework by jointly inferring their posteriors. This enables individual tasks to fully leverage inductive biases provided by related tasks, therefore improving the overall performance of all tasks. Experimental results demonstrate that the proposed VMTL is able to effectively tackle a variety of challenging multi-task learning problems with limited training data for both classiﬁcation and regression. Our method consistently surpasses previous methods, including strong Bayesian approaches, and achieves state-of-the-art performance on ﬁve benchmark datasets.\n","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"593f7c6a8298f8da3debe64610c9b8c6","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-nips-jiayi/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/aimlab/post/2021-nips-jiayi/","section":"post","summary":"Congratulations on Jiayi Shen\u0026rsquo;s paper \u0026ldquo;Variational Multi-Task Learning with Gumbel-Softmax Priors\u0026rdquo; being accepted by NeurIPS 2021.\n","tags":null,"title":"One paper accepted at NeurIPS 2021.","type":"post"},{"authors":["Jiayi Shen","Xiantong Zhen","Marcel Worring","Ling Shao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"03cb4411b246d763e69174a1e5736252","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-nips2021-jiayi/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-nips2021-jiayi/","section":"publication","summary":"We propose variational multi-task learning (VMTL), a general probabilistic inference framework in which we cast multi-task learning as a variational Bayesian inference problem.","tags":["Source Themes"],"title":"Variational Multi-Task Learning with Gumbel-Softmax Priors","type":"publication"},{"authors":null,"categories":null,"content":"The AIM Lab opened on 31 January 2020. An update by Assistant Professor Xiantong Zhen at the AIM Lab. AIM Lab is a collaborative initiative of the Inception Institute of Artificial Intelligence from the United Arab Emirates and the University of Amsterdam.\nThe research lab is focused on medical image analysis by machine learning, covering active scientific topics of broad interest, including both methods and applications. These topics range from low-level vision and data pre-processing tasks to high-level image/video analysis tasks. From a technical perspective, researchers at the AIM Lab will be working on fundamental and relatively general deep-learning models and algorithms, which will be applied to specific diseases, including but not limited to Alzheimer’s disease, cancer and cardiovascular diseases. In particular, the AIM Lab will be focusing on the following seven projects in its first five years:\n• Learning with limited data and its applications to medical image analysis\n• Multi-task learning for medical image analysis and data mining\n• Continual learning with its applications to medical image classification\n• Out-of-distribution generalization for medical image analysis\n• Jointly learning from medical images and health records\n• Automated report generation from radiology images.\nThe detailed news could be found in: https://ivi.uva.nl/content/news/2021/07/artificial-intelligence-for-medical-imaging-lab.html\n","date":1630540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630540800,"objectID":"f68072f9c35c2c4fe5ea7e97ed21f61c","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-acl-yingjun/","publishdate":"2021-09-02T00:00:00Z","relpermalink":"/aimlab/post/2021-acl-yingjun/","section":"post","summary":"The AIM Lab opened on 31 January 2020. An update by Assistant Professor Xiantong Zhen at the AIM Lab. AIM Lab is a collaborative initiative of the Inception Institute of Artificial Intelligence from the United Arab Emirates and the University of Amsterdam.\n","tags":null,"title":"Artificial Intelligence for Medical Imaging Lab.","type":"post"},{"authors":null,"categories":null,"content":"Recently, an article from researchers from the Informatics Institute of the University of Amsterdam has been published at the IEEE Conference on Computer Vision and Pattern Recognition; the leading peer-reviewed publication venue in the field of artificial intelligence. The researchers introduced a method for counting repetitions, which are relevant when analyzing human activity (sports), animal behavior (a bee’s waggle dance) or natural phenomena (leaves in the wind) by integrating for the first time the audio modality in a visual counting system based on neural networks.\nYunhua Zhang and Cees Snoek of the Video \u0026amp; Image Sense Lab (VIS), in collaboration with Ling Shao from the Inception Institute of Artificial Intelligence (IIAI), developed a method for estimating how many times a certain repetitive phenomenon, such as bouncing on a trampoline, slicing an onion, or playing ping pong, happens in a video stream. Their methodology is applicable to any scenario in which repetitive motion patterns exist. By using both sight and sound, as well as their cross-modal interaction, counting predictions are shown to be much more robust than a sight-only model. The results highlight the benefits brought by the use of sound as an addition to sight, especially in harsh vision conditions, for example during low illumination, or when camera viewpoint changes, and even occlusions, where the combination of both sight and sound always outperforms the use of sight only.\nThe detailed information could be found in https://www.uva.nl/en/shared-content/subsites/informatics-institute/en/news/2021/06/adding-ears-to-a-computer-counting.html.\n","date":1625184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625184000,"objectID":"d6eaf35c27f77a8446ec63626fa6c811","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-cvpr2-yunhua/","publishdate":"2021-07-02T00:00:00Z","relpermalink":"/aimlab/post/2021-cvpr2-yunhua/","section":"post","summary":"Recently, an article from researchers from the Informatics Institute of the University of Amsterdam has been published at the IEEE Conference on Computer Vision and Pattern Recognition; the leading peer-reviewed publication venue in the field of artificial intelligence. The researchers introduced a method for counting repetitions, which are relevant when analyzing human activity (sports), animal behavior (a bee’s waggle dance) or natural phenomena (leaves in the wind) by integrating for the first time the audio modality in a visual counting system based on neural networks.\n","tags":null,"title":"Adding ears to a computer counting the bounces on a trampoline.","type":"post"},{"authors":[],"categories":null,"content":"","date":1623430800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623430800,"objectID":"a82983462174ad33ff8eca7a977d9d9d","permalink":"https://ivi.fnwi.uva.nl/aimlab/event/first-dinner/","publishdate":"2021-06-11T00:00:00Z","relpermalink":"/aimlab/event/first-dinner/","section":"event","summary":"First offline AIM Lab gathering.","tags":[],"title":"Example Event","type":"event"},{"authors":[],"categories":null,"content":"","date":1623430800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623430800,"objectID":"a2096dad29ccd4295579ccf014473a0d","permalink":"https://ivi.fnwi.uva.nl/aimlab/event/first-offline-aim-lab-gathering/","publishdate":"2021-06-11T00:00:00Z","relpermalink":"/aimlab/event/first-offline-aim-lab-gathering/","section":"event","summary":"First offline AIM Lab gathering.","tags":[],"title":"Example Event","type":"event"},{"authors":null,"categories":null,"content":"Congratulations on Mohammad Derakhshani\u0026rsquo;s paper \u0026ldquo;Kernel Continual Learning\u0026rdquo; being accepted by ICML 2021.\nThis paper introduces kernel continual learning, a simple but effective variant of continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. We deploy an episodic memory unit that stores a subset of samples for each task to learn task-speciﬁc classiﬁers based on kernel ridge regression. This does not require memory replay and systematically avoids task interference in the classiﬁers. We further introduce variational random features to learn a data-driven kernel for each task. To do so, we formulate kernel continual learning as a variational inference problem, where a random Fourier basis is incorporated as the latent variable. The variational posterior distribution over the random Fourier basis is inferred from the coreset of each task. In this way, we are able to generate more informative kernels speciﬁc to each task, and, more importantly, the coreset size can be reduced to achieve more compact memory, resulting in more efﬁcient continual learning based on episodic memory. Extensive evaluation on four benchmarks demonstrates the effectiveness and promise of kernels for continual learning.\n","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620604800,"objectID":"5c68de04bffdfb6db9ec0ed3807e8ce5","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-icml-mohanmad/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/aimlab/post/2021-icml-mohanmad/","section":"post","summary":"Congratulations on Mohammad Derakhshani\u0026rsquo;s paper \u0026ldquo;Kernel Continual Learning\u0026rdquo; being accepted by ICML 2021.\n","tags":null,"title":"One paper accepted at ICML 2021.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations on Zehao Xiao\u0026rsquo;s paper \u0026ldquo;A Bit More Bayesian: Domain-Invariant Learning with Uncertainty\u0026rdquo; being accepted by ICML 2021.\nThe ICML 2021 paper “A Bit More Bayesian: Domain-Invariant Learning with Uncertaintyce” by Zehao Xiao, Jiayi Shen, Xiantong Zhen, Ling Shao and Cees Snoek is now available. Domain generalization is challenging due to the domain shift and the uncertainty caused by the inaccessibility of target domain data. In this paper, we address both challenges with a probabilistic framework based on variational Bayesian inference, by incorporating uncertainty into neural network weights. We couple domain invariance in a probabilistic formula with the variational Bayesian inference. This enables us to explore domain-invariant learning in a principled way. Specifically, we derive domain-invariant representations and classifiers, which are jointly established in a two-layer Bayesian neural network. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies validate the synergistic benefits of our Bayesian treatment when jointly learning domain-invariant representations and classifiers for domain generalization. Further, our method consistently delivers state-of-the-art mean accuracy on all benchmarks.\n","date":1620518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620518400,"objectID":"e4478c96cefecaaef9e433d621a0ff79","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-icml-zehao/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/aimlab/post/2021-icml-zehao/","section":"post","summary":"Congratulations on Zehao Xiao\u0026rsquo;s paper \u0026ldquo;A Bit More Bayesian: Domain-Invariant Learning with Uncertainty\u0026rdquo; being accepted by ICML 2021.\n","tags":null,"title":"One paper accepted at ICML 2021.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations on Ivona Najdenkoska\u0026rsquo;s paper \u0026ldquo;Variational Topic Inference for Chest X-Ray Report Generation\u0026rdquo; being accepted by MICCAI 2021.\nAutomating report generation for medical imaging promises to alleviate workload and assist diagnosis in clinical practice. Recent work has shown that deep learning models can successfully caption natural images. However, learning from medical data is challenging due to the diversity and uncertainty inherent in the reports written by diﬀerent radiologists with discrepant expertise and experience. To tackle these challenges, we propose variational topic inference for automatic report generation. Speciﬁcally, we introduce a set of topics as latent variables to guide sentence generation by aligning image and language modalities in a latent space. The topics are inferred in a conditional variational inference framework, with each topic governing the generation of a sentence in the report. Further, we adopt a visual attention module that enables the model to attend to diﬀerent local regions in the image to generate more informative descriptions. We conduct extensive experiments on two benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results demonstrate that our proposed variational topic inference model can generate reports which are not mere copies of reports used in training, while still achieving comparable performance to state-of-the-art methods in terms of standard language generation criteria.\n","date":1620000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620000000,"objectID":"64df48ec12455bf3fe52b0082397b2e3","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-miccai-ivona/","publishdate":"2021-05-03T00:00:00Z","relpermalink":"/aimlab/post/2021-miccai-ivona/","section":"post","summary":"Congratulations on Ivona Najdenkoska\u0026rsquo;s paper \u0026ldquo;Variational Topic Inference for Chest X-Ray Report Generation\u0026rdquo; being accepted by MICCAI 2021.\n","tags":null,"title":"One paper accepted at MICCAI 2021.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations on Ivona Najdenkoska\u0026rsquo;s paper \u0026ldquo;Variational Topic Inference for Chest X-Ray Report Generation\u0026rdquo; won the MICCAI Student Travel Awards at MICCAI 2021.\nThe MICCAI Student Travel Awards scheme serves two main functions:\n  To reward the best (such as highest scoring) first author students and to subsidise their attendance to the present their work at the annual MICCAI conference.\n  To assist in building the MICCAI community by selecting early career participants from countries of lower-income countries from where fewer papers are received and the authors are typically not well funded to attend international meetings.\n  The detailed information could be found in the http://miccai.org/about-miccai/awards/student-travel-awards/.\n","date":1619913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619913600,"objectID":"191ad538a01c540823b512de5a60cd99","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-travel-award-ivona/","publishdate":"2021-05-02T00:00:00Z","relpermalink":"/aimlab/post/2021-travel-award-ivona/","section":"post","summary":"Congratulations on Ivona Najdenkoska\u0026rsquo;s paper \u0026ldquo;Variational Topic Inference for Chest X-Ray Report Generation\u0026rdquo; won the MICCAI Student Travel Awards at MICCAI 2021.\n","tags":null,"title":"One paper won the MICCAI Student Travel Awards at MICCAI 2021.","type":"post"},{"authors":["Yingjun Du","Nithin Holla","Xiantong Zhen","Cees G.M. Snoek","Ekaterina Shutova",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"19e913229dedb14cfd2c60e98c0d4a37","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-acl2021-yingjun/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-acl2021-yingjun/","section":"publication","summary":"We propose  a model of semantic memory for WSD in a meta-learning setting.","tags":["Source Themes"],"title":"Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation","type":"publication"},{"authors":["Ivona Najdenkoska","Xiantong Zhen","Marcel Worring","Ling Shao",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"53e7032201326eec5ddc805b44537584","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-miccai2021-ivona/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-miccai2021-ivona/","section":"publication","summary":"We propose  variational topic inference for automatic report generation.","tags":["Source Themes"],"title":"Variational Topic Inference for Chest X-Ray Report Generation","type":"publication"},{"authors":["Zehao Xiao","Jiayi Shen","Xiantong Zhen","Ling Shao","Cees G.M. Snoek"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"72e3a877fa7e29b6bb64381111c730a8","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2021-zehao/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-icml2021-zehao/","section":"publication","summary":"In this paper, we address the challenges of domain shift and the uncertainty with a probabilistic framework based on variational Bayesian inference, by incorporating uncertainty into neural network weights.","tags":["Source Themes"],"title":"A Bit More Bayesian: Domain-Invariant Learning with Uncertainty","type":"publication"},{"authors":["Mohammad Derakhshani","Xiantong Zhen","Ling Shao","Cees G.M. Snoek"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"6f64bbcd3961820b0e6320dc5905c82f","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2021-mohammad/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-icml2021-mohammad/","section":"publication","summary":"We deploy an episodic memory unit that stores a subset of samples for each task to learn task-speciﬁc classiﬁers based on kernel ridge regression.","tags":["Source Themes"],"title":"Kernel Continual Learning","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations on Yunhua Zhang\u0026rsquo;s paper \u0026ldquo;Repetitive Activity Counting by Sight and Sound\u0026rdquo; being accepted by CVPR 2021.\nThis paper strives for repetitive activity counting in videos. Different from existing works, which all analyze the visual video content only, we incorporate for the first time the corresponding sound into the repetition counting process. This benefits accuracy in challenging vision conditions such as occlusion, dramatic camera view changes, low resolution, etc. We propose a model that starts with analyzing the sight and sound streams separately. Then an audiovisual temporal stride decision module and a reliability estimation module are introduced to exploit cross-modal temporal interaction. For learning and evaluation, an existing dataset is repurposed and reorganized to allow for repetition counting with sight and sound. We also introduce a variant of this dataset for repetition counting under challenging vision conditions. Experiments demonstrate the benefit of sound, as well as the other introduced modules, for repetition counting. Our sight-only model already outperforms the state-of-the-art by itself, when we add sound, results improve notably, especially under harsh vision conditions.\n","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"f04c95b19cde36c2541ff24813312ea0","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-cvpr-yunhua/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/aimlab/post/2021-cvpr-yunhua/","section":"post","summary":"Congratulations on Yunhua Zhang\u0026rsquo;s paper \u0026ldquo;Repetitive Activity Counting by Sight and Sound\u0026rdquo; being accepted by CVPR 2021.\n","tags":null,"title":"One paper accepted at CVPR 2021.","type":"post"},{"authors":["Yingjun Du","Xiantong Zhen","Ling Shao","Cees G.M. Snoek",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"c7b7e5bb066a88fc1327ff11028686a9","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-iclr2021-yingjun/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-iclr2021-yingjun/","section":"publication","summary":"We propose MetaNorm, a simple yet effective meta-learning normalization approach that learns adaptive statistics for few-shot classification and domain generalization.","tags":["Source Themes"],"title":"MetaNorm: Learning to Normalize Few-Shot Batches Across Domains","type":"publication"},{"authors":["Yunhua Zhang","Ling Shao","Cees G.M. Snoek",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"34bbeb3cae70c5c77acb57e3bb405dd3","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-cvpr2021-yunhua/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-cvpr2021-yunhua/","section":"publication","summary":"This paper strives for repetitive activity counting in videos. Different from existing works, which all analyze the visual video content only, we incorporate for the first time the corresponding sound into the repetition counting process.","tags":["Source Themes"],"title":"Repetitive Activity Counting by Sight and Sound","type":"publication"},{"authors":["Haochen Wang","Yandan Yang","Xianbin Cao","Xiantong Zhen","Cees G.M. Snoek","Ling Shao",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"7be164187a3807d5956f1e94a30d3da2","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-wacv2021-haochen/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-wacv2021-haochen/","section":"publication","summary":"This paper propose variational prototype inference to address few-shot semantic segmentation in a probabilistic framework.","tags":["Source Themes"],"title":"Variational Prototype Inference for Few-Shot Semantic Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations on Tom van Sonsbeek\u0026rsquo;s paper \u0026ldquo;Variational Knowledge Distillation for Disease Classification in Chest X-Rays\u0026rdquo; being accepted by IPMI 2021.\nDisease classification relying solely on imaging data attracts great interest in medical image analysis. Current models could be fur- ther improved, however, by also employing Electronic Health Records (EHRs), which contain rich information on patients and findings from clinicians. It is challenging to incorporate this information into disease classification due to the high reliance on clinician input in EHRs, lim- iting the possibility for automated diagnosis. In this paper, we propose variational knowledge distillation (VKD), which is a new probabilistic inference framework for disease classification based on X-rays that lever- ages knowledge from EHRs. Specifically, we introduce a conditional la- tent variable model, where we infer the latent representation of the X-ray image with the variational posterior conditioning on the associated EHR text. By doing so, the model acquires the ability to extract the visual features relevant to the disease during learning and can therefore perform more accurate classification for unseen patients at inference based solely on their X-ray scans. We demonstrate the effectiveness of our method on three public benchmark datasets with paired X-ray images and EHRs. The results show that the proposed variational knowledge distillation can consistently improve the performance of medical image classification and significantly surpasses current methods.\n","date":1612224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612224000,"objectID":"00f4227cd76c47cd51c2a9262bbf7102","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-ipmi-tom/","publishdate":"2021-02-02T00:00:00Z","relpermalink":"/aimlab/post/2021-ipmi-tom/","section":"post","summary":"Congratulations on Tom van Sonsbeek\u0026rsquo;s paper \u0026ldquo;Variational Knowledge Distillation for Disease Classification in Chest X-Rays\u0026rdquo; being accepted by IPMI 2021.\n","tags":null,"title":"One paper accepted at IPMI 2021.","type":"post"},{"authors":["Tom van Sonsbeek","Xiantong Zhen","Marcel Worring","Ling Shao",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"e8ec1dd87fe9dd9847fc2f8e63cb9a07","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-ipmi2021-tom/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-ipmi2021-tom/","section":"publication","summary":"We propose variational knowledge distillation (VKD), which is a new probabilistic inference framework for disease classification based on X-rays that lever- ages knowledge from EHRs.","tags":["Source Themes"],"title":"Variational Knowledge Distillation for Disease Classification in Chest X-Rays","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations on Yingjun Du\u0026rsquo;s paper \u0026ldquo;MetaNorm: Learning to Normalize Few-Shot Batches Across Domains\u0026rdquo; being accepted by ICLR 2021.\nBatch normalization plays a crucial role when training deep neural networks. However, batch statistics become unstable with small batch sizes and are unreliable in the presence of distribution shifts. We propose MetaNorm, a simple yet effective meta-learning normalization. It tackles the aforementioned issues in a uniﬁed way by leveraging the meta-learning setting and learns to infer adaptive statistics for batch normalization. MetaNorm is generic, ﬂexible and model-agnostic, making it a simple plug-and-play module that is seamlessly embedded into existing meta-learning approaches. It can be efﬁciently implemented by lightweight hypernetworks with low computational cost. We verify its effectiveness by extensive evaluation on representative tasks suffering from the small batch and domain shift problems: few-shot learning and domain generalization. We further introduce an even more challenging setting: few-shot domain generalization. Results demonstrate that MetaNorm consistently achieves better, or at least competitive, accuracy compared to existing batch normalization methods.\n","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"7fe506da260dc148e48c2f7151f234b4","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2021-iclr-yingjun/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/aimlab/post/2021-iclr-yingjun/","section":"post","summary":"Congratulations on Yingjun Du\u0026rsquo;s paper \u0026ldquo;MetaNorm: Learning to Normalize Few-Shot Batches Across Domains\u0026rdquo; being accepted by ICLR 2021.\n","tags":null,"title":"One paper accepted at ICLR 2021.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations on Xiantong Zhen\u0026rsquo;s paper \u0026ldquo;Learning to Learn Variational Semantic Memory\u0026rdquo; being published by NeurIPS 2020.\nIn this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efﬁciently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the beneﬁt of variational semantic memory in boosting few-shot recognition.\n","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"4f08b1e49276f51cb9836c41f69e7394","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2020-nips-xiantong/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/aimlab/post/2020-nips-xiantong/","section":"post","summary":"Congratulations on Xiantong Zhen\u0026rsquo;s paper \u0026ldquo;Learning to Learn Variational Semantic Memory\u0026rdquo; being published by NeurIPS 2020.\n","tags":null,"title":"One paper accepted at NeurIPS 2020.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations on Yingjun Du\u0026rsquo;s paper \u0026ldquo;Learning to Learn with Variational Information Bottleneck for Domain Generalization\u0026rdquo; being accepted by ICML 2020.\nDomain generalization models learn to generalize to previously unseen domains, but suffer from prediction uncertainty and domain shift. In this paper, we address both problems. We introduce a probabilistic meta-learning model for domain generalization, in which classifier parameters shared across domains are modeled as distributions. This enables better handling of prediction uncertainty on unseen domains. To deal with domain shift, we learn domain-invariant representations by the proposed principle of meta variational information bottleneck, we call MetaVIB. MetaVIB is derived from novel variational bounds of mutual information, by leveraging the meta-learning setting of domain generalization. Through episodic training, MetaVIB learns to gradually narrow domain gaps to establish domain-invariant representations, while simultaneously maximizing prediction accuracy. We conduct experiments on three benchmarks for cross-domain visual recognition. Comprehensive ablation studies validate the benefits of MetaVIB for domain generalization. The comparison results demonstrate our method outperforms previous approaches consistently.\n","date":1600387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600387200,"objectID":"0e3d9724544c799ac5d038b7ac1b0ddf","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2020-eccv-yingjun/","publishdate":"2020-09-18T00:00:00Z","relpermalink":"/aimlab/post/2020-eccv-yingjun/","section":"post","summary":"Congratulations on Yingjun Du\u0026rsquo;s paper \u0026ldquo;Learning to Learn with Variational Information Bottleneck for Domain Generalization\u0026rdquo; being accepted by ICML 2020.\n","tags":null,"title":"One paper accepted at ECCV 2020.","type":"post"},{"authors":["Haochen Wang","Xudong Zhang","Yutao Hu","Yandan Yang","Xianbin Cao","Xiantong Zhen",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1599004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599004800,"objectID":"2ef6ca1774247e1776c6cdcddb14d28b","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-eccv2020-xiantong/","publishdate":"2020-09-02T00:00:00Z","relpermalink":"/aimlab/publication/conference-eccv2020-xiantong/","section":"publication","summary":"Few-Shot Segmentation, Graph Attention, Democratic Attention Network, Multi-Scale Guidance","tags":["Source Themes"],"title":"Few-Shot Semantic Segmentation with Democratic Attention Networks","type":"publication"},{"authors":["Yingjun Du","Jun Xu","Huan Xiong","Qiang Qiu","Xiantong Zhen","Cees G.M. Snoek","Ling Shao",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"d066d9c8ab04d5eb0c8723d31531fa6b","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-eccv2020-yingjun/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-eccv2020-yingjun/","section":"publication","summary":"Meta Learning, Domain Generalization, Variational Inference, Information Bottleneck","tags":["Source Themes"],"title":"Learning to Learn with Variational Information Bottleneck for Domain Generalization","type":"publication"},{"authors":["Mengshi Qi","Jie Qin","Xiantong Zhen","Di Huang","Yi Yang","Jiebo Luo",""],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"eb64e99f8279515fb3620695474d3770","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-acm2020-xiantong/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-acm2020-xiantong/","section":"publication","summary":"In this paper, we address few-shot video classification by learning an ensemble of SlowFast networks augmented with memory units.","tags":["Source Themes"],"title":"Few-Shot Ensemble Learning for Video Classification with SlowFast Memory Networks","type":"publication"},{"authors":["Xiantong Zhen","Yingjun Du","Huan Xiong","Cees G.M. Snoek","Ling Shao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"ca0b33950b8e355d871b735cc4ae8d18","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-nips2020-xiantong/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-nips2020-xiantong/","section":"publication","summary":"In this work, we introduce kernels with random Fourier features in the meta-learning framework to leverage their strong few-shot learning ability.","tags":["Source Themes"],"title":"Learning to Learn Variational Semantic Memory","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations on Xiantong Zhen\u0026rsquo;s paper \u0026ldquo;Learning to Learn Kernels with Variational Random Features\u0026rdquo; being accepted by ICML 2020.\nWe introduce kernels with random Fourier features in the meta-learning framework for few-shot learning. We propose meta variational random features (MetaVRF) to learn adaptive kernels for the base-learner, which is developed in a latent variable model by treating the random feature basis as the latent variable. We formulate the optimization of MetaVRF as a variational inference problem by deriving an evidence lower bound under the meta-learning framework. To incorporate shared knowledge from related tasks, we propose a context inference of the posterior, which is established by an LSTM architecture. The LSTMbased inference network effectively integrates the context information of previous tasks with taskspecific information, generating informative and adaptive features. The learned MetaVRF is able to produce kernels of high representational power with a relatively low spectral sampling rate and also enables fast adaptation to new tasks. Experimental results on a variety of few-shot regression and classification tasks demonstrate that MetaVRF can deliver much better, or at least competitive, performance compared to existing metalearning alternatives.\n","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588982400,"objectID":"2c3328b2ebcc0c697a9faae11ee9b46d","permalink":"https://ivi.fnwi.uva.nl/aimlab/post/2020-icml-xiantong/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/aimlab/post/2020-icml-xiantong/","section":"post","summary":"Congratulations on Xiantong Zhen\u0026rsquo;s paper \u0026ldquo;Learning to Learn Kernels with Variational Random Features\u0026rdquo; being accepted by ICML 2020.\n","tags":null,"title":"One paper accepted at ICML 2020.","type":"post"},{"authors":["Zhiang Liu","Yingkun Hou","Xiantong Zhen","Jun Xu","Ling Shao","MingMing Cheng"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1583193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583193600,"objectID":"84df2f05a823c11ec775566bd6bb6432","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/journal-acm2020-xiantong/","publishdate":"2020-03-03T00:00:00Z","relpermalink":"/aimlab/publication/journal-acm2020-xiantong/","section":"publication","summary":"Image smoothing, benchmark dataset, performance evaluation, pixel-level non-local self similarity.","tags":["Source Themes"],"title":"Pixel-level Non-local Image Smoothing with Objective Evaluation","type":"publication"},{"authors":["Yingjun Du","Jun Xu","Xiantong Zhen","MingMing Cheng","Ling Shao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"7e89d6841b4c64cd167235b422971bac","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/journal-tip2020-yingjun/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/aimlab/publication/journal-tip2020-yingjun/","section":"publication","summary":"In this paper, we propose a Conditional Variational Image Deraining (CVID) network for better deraining performance, leveraging the exclusive generative ability of Conditional Variational Auto-Encoder (CVAE) on providing diverse predictions for the rainy image.","tags":["Source Themes"],"title":"Conditional Variational Image Deraining","type":"publication"},{"authors":["Xiantong Zhen","Haoliang Sun","Yingjun Du","Jun Xu","Yilong Yin","Ling Shao","Cees G.M. Snoek"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"3cd82760ddef6965efb4743dd565b701","permalink":"https://ivi.fnwi.uva.nl/aimlab/publication/conference-icml2020-xiantong/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/aimlab/publication/conference-icml2020-xiantong/","section":"publication","summary":"In this work, we introduce kernels with random Fourier features in the meta-learning framework to leverage their strong few-shot learning ability.","tags":["Source Themes"],"title":"Learning to Learn Kernels with Variational Random Features","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://ivi.fnwi.uva.nl/aimlab/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ivi.fnwi.uva.nl/aimlab/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aimlab/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]